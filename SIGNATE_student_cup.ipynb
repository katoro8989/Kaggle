{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/katoro8989/Kaggle/blob/signate/SIGNATE_student_cup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFvg9l8NEgh5"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import log_loss\n",
        "# from catboost import Pool, CatBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import lightgbm as lgbm\n",
        "\n",
        "import xgboost as xgb\n",
        "# from tabpfn import TabPFNClassifier\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from keras import callbacks\n",
        "from keras import optimizers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.layers import PReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "# get datasets\n",
        "train = pd.read_csv('/content/drive/MyDrive/SIGNATE/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/SIGNATE/test.csv')\n",
        "sample = pd.read_csv('/content/drive/MyDrive/SIGNATE/submit_sample.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zALlitvbUA7"
      },
      "source": [
        "# 前処理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGONn3BMZXmL"
      },
      "outputs": [],
      "source": [
        "def pre_process(train, test):\n",
        "  # year\n",
        "  len_train = len(train)\n",
        "  for id in train[train['year'] >= 2023]['id']:\n",
        "    train[id:id+1]['year'] -= 1000\n",
        "\n",
        "  for id in test[test['year'] >= 2023]['id']:\n",
        "    test[id-len_train:id+1-len_train]['year'] -= 1000\n",
        "\n",
        "  # odometer\n",
        "  # trainに\n",
        "  # 1000000キロ？以上のポイントが存在、マイルでもおかしい\n",
        "  # -1も結構ある\n",
        "\n",
        "  train.loc[train['odometer'] >= 1e6, 'odometer'] //= 10\n",
        "  test.loc[test['odometer'] >= 1e6, 'odometer'] //= 10\n",
        "\n",
        "  train.loc[train['odometer'] < 0, 'odometer'] = np.nan\n",
        "  test.loc[test['odometer'] < 0, 'odometer'] = np.nan\n",
        "\n",
        "  # region\n",
        "  # カテゴリごとのpriceの平均値を計算\n",
        "  price_mean_by_region = train.groupby('region')['price'].mean().reset_index()\n",
        "\n",
        "  # priceの平均値が高い順にラベルを付与\n",
        "  price_mean_by_region = price_mean_by_region.sort_values(by='price', ascending=True)\n",
        "  price_mean_by_region['region_label'] = range(len(price_mean_by_region))\n",
        "\n",
        "  # ラベルを元のデータにマージ\n",
        "  train = train.merge(price_mean_by_region[['region', 'region_label']], on='region', how='left')\n",
        "  test = test.merge(price_mean_by_region[['region', 'region_label']], on='region', how='left')\n",
        "\n",
        "  # manufucturer\n",
        "\n",
        "  import unicodedata\n",
        "\n",
        "  train['manufacturer'] = train['manufacturer'].str.replace(' ', '')\n",
        "  train['manufacturer'] = train['manufacturer'].apply(lambda x: unicodedata.normalize('NFKC', x))\n",
        "  train['manufacturer'] = train['manufacturer'].str.lower()\n",
        "  train.loc[train[\"manufacturer\"] == \"niѕsan\", \"manufacturer\"] = \"nissan\"\n",
        "  train.loc[train[\"manufacturer\"] == \"nisѕan\", \"manufacturer\"] = \"nissan\"\n",
        "  train.loc[train[\"manufacturer\"] == \"subαru\", \"manufacturer\"] = \"subaru\"\n",
        "  train.loc[train[\"manufacturer\"] == \"toyotа\", \"manufacturer\"] = \"toyota\"\n",
        "  train.loc[train[\"manufacturer\"] == \"sαturn\", \"manufacturer\"] = \"saturn\"\n",
        "  train.loc[train[\"manufacturer\"] == \"аcura\", \"manufacturer\"] = \"acura\"\n",
        "  train.loc[train[\"manufacturer\"] == \"vоlkswagen\", \"manufacturer\"] = \"volkswagen\"\n",
        "  train.loc[train[\"manufacturer\"] == \"lexuѕ\", \"manufacturer\"] = \"lexus\"\n",
        "  train.loc[train[\"manufacturer\"] == \"ᴄhrysler\", \"manufacturer\"] = \"chrysler\"\n",
        "\n",
        "  test['manufacturer'] = test['manufacturer'].str.replace(' ', '')\n",
        "  test['manufacturer'] = test['manufacturer'].apply(lambda x: unicodedata.normalize('NFKC', x))\n",
        "  test['manufacturer'] = test['manufacturer'].str.lower()\n",
        "  test.loc[test[\"manufacturer\"] == \"niѕsan\", \"manufacturer\"] = \"nissan\"\n",
        "  test.loc[test[\"manufacturer\"] == \"nisѕan\", \"manufacturer\"] = \"nissan\"\n",
        "  test.loc[test[\"manufacturer\"] == \"subαru\", \"manufacturer\"] = \"subaru\"\n",
        "  test.loc[test[\"manufacturer\"] == \"toyotа\", \"manufacturer\"] = \"toyota\"\n",
        "  test.loc[test[\"manufacturer\"] == \"sαturn\", \"manufacturer\"] = \"saturn\"\n",
        "  test.loc[test[\"manufacturer\"] == \"аcura\", \"manufacturer\"] = \"acura\"\n",
        "  test.loc[test[\"manufacturer\"] == \"vоlkswagen\", \"manufacturer\"] = \"volkswagen\"\n",
        "  test.loc[test[\"manufacturer\"] == \"lexuѕ\", \"manufacturer\"] = \"lexus\"\n",
        "  test.loc[test[\"manufacturer\"] == \"ᴄhrysler\", \"manufacturer\"] = \"chrysler\"\n",
        "\n",
        "  # カテゴリごとのpriceの平均値を計算\n",
        "  price_mean_by_region = train.groupby('manufacturer')['price'].mean().reset_index()\n",
        "\n",
        "  # priceの平均値が高い順にラベルを付与\n",
        "  price_mean_by_region = price_mean_by_region.sort_values(by='price', ascending=True)\n",
        "  price_mean_by_region['manufacturer_label'] = range(len(price_mean_by_region))\n",
        "\n",
        "  # ラベルを元のデータにマージ\n",
        "  train = train.merge(price_mean_by_region[['manufacturer', 'manufacturer_label']], on='manufacturer', how='left')\n",
        "  test = test.merge(price_mean_by_region[['manufacturer', 'manufacturer_label']], on='manufacturer', how='left')\n",
        "\n",
        "  # condition\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      'excellent': 4,\n",
        "      'good': 3,\n",
        "      'like new': 5,\n",
        "      'fair': 2,\n",
        "      'new': 6,\n",
        "      'salvage': 1,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['condition_label'] = train['condition'].map(condition_labels)\n",
        "  test['condition_label'] = test['condition'].map(condition_labels)\n",
        "\n",
        "  # cylinders\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      \"6 cylinders\": 6,\n",
        "      \"4 cylinders\": 4,\n",
        "      \"8 cylinders\": 8,\n",
        "      \"other\": -30,\n",
        "      \"10 cylinders\": 10,\n",
        "      \"5 cylinders\": 5,\n",
        "      \"3 cylinders\": 3,\n",
        "      \"12 cylinders\": 12,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['cylinders_label'] = train['cylinders'].map(condition_labels)\n",
        "  test['cylinders_label'] = test['cylinders'].map(condition_labels)\n",
        "\n",
        "  # fuel\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      \"gas\": 2,\n",
        "      \"diesel\": 5,\n",
        "      \"hybrid\": 1,\n",
        "      \"other\": 3,\n",
        "      \"electric\": 4,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['fuel_label'] = train['fuel'].map(condition_labels)\n",
        "  test['fuel_label'] = test['fuel'].map(condition_labels)\n",
        "\n",
        "  # titel_status\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      \"clean\": 5,\n",
        "      \"rebuilt\": 4,\n",
        "      \"salvage\": 3,\n",
        "      \"lien\": 6,\n",
        "      \"parts only\": 2,\n",
        "      \"missing\": 1,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['title_status_label'] = train['title_status'].map(condition_labels)\n",
        "  test['title_status_label'] = test['title_status'].map(condition_labels)\n",
        "\n",
        "  # transmission\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      \"automatic\": 3,\n",
        "      \"manual\": 1,\n",
        "      \"other\": 2,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['transmission_label'] = train['transmission'].map(condition_labels)\n",
        "  test['transmission_label'] = test['transmission'].map(condition_labels)\n",
        "\n",
        "  # drive\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      \"fwd\": 1,\n",
        "      \"4wd\": 3,\n",
        "      \"rwd\": 2,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['drive_label'] = train['drive'].map(condition_labels)\n",
        "  test['drive_label'] = test['drive'].map(condition_labels)\n",
        "\n",
        "  # size\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      'full-size': 4,\n",
        "      'mid-size': 3,\n",
        "      'compact': 1,\n",
        "      'sub-compact': 2,\n",
        "      'midーsize': 3,\n",
        "      'fullーsize': 4,\n",
        "      'mid−size': 3,\n",
        "      'full−size': 4,\n",
        "      'subーcompact': 2,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['size_label'] = train['size'].map(condition_labels)\n",
        "  test['size_label'] = test['size'].map(condition_labels)\n",
        "\n",
        "  # type\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      'truck': 13,\n",
        "      'pickup': 12,\n",
        "      'bus': 11,\n",
        "      'van': 10,\n",
        "      'offroad': 9,\n",
        "      'SUV': 8,\n",
        "      'convertible': 7,\n",
        "      'other': 6,\n",
        "      'coupe': 5,\n",
        "      'mini-van': 4,\n",
        "      'sedan': 3,\n",
        "      'hatchback': 2,\n",
        "      'wagon': 1,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['type_label'] = train['type'].map(condition_labels)\n",
        "  test['type_label'] = test['type'].map(condition_labels)\n",
        "\n",
        "  # paint_color\n",
        "  # 状態ごとに割り当てるラベルを定義\n",
        "  condition_labels = {\n",
        "      'white': 12,\n",
        "      'black': 11,\n",
        "      'yellow': 10,\n",
        "      'custom': 9,\n",
        "      'blue': 8,\n",
        "      'grey': 7,\n",
        "      'red': 6,\n",
        "      'silver': 5,\n",
        "      'orange': 4,\n",
        "      'purple': 3,\n",
        "      'brown': 2,\n",
        "      'green': 1,\n",
        "  }\n",
        "\n",
        "  # ラベルエンコーディングを実行\n",
        "  train['paint_color_label'] = train['paint_color'].map(condition_labels)\n",
        "  test['paint_color_label'] = test['paint_color'].map(condition_labels)\n",
        "\n",
        "  return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYE5VPP8avRM"
      },
      "outputs": [],
      "source": [
        "train, test = pre_process(train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0g2ozaBENGQ"
      },
      "outputs": [],
      "source": [
        "#説明変数と目的変数に分ける\n",
        "train_y = train.price\n",
        "train_x = train.drop(columns=[\"price\", \"id\", \"region\", \"manufacturer\", \"condition\", \"cylinders\", 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'])\n",
        "test_x = test.drop(columns=[\"id\", \"region\", \"manufacturer\", \"condition\", \"cylinders\", 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'])\n",
        "\n",
        "#積と商によって特徴量作成\n",
        "original_features = train_x.columns\n",
        "\n",
        "for column in original_features:\n",
        "  if column not in [\"odometer\"]:\n",
        "    train_x['{}_div'.format(column)] = train_x[\"odometer\"] / (train_x[column] + np.finfo(float).eps)\n",
        "    test_x['{}_div'.format(column)] = test_x[\"odometer\"] / (test_x[column] + np.finfo(float).eps)\n",
        "\n",
        "for column in original_features:\n",
        "  if column not in [\"year\", \"odometer\"]:\n",
        "    train_x['{}_mul'.format(column)] = train_x[\"year\"] * (train_x[column] + np.finfo(float).eps)\n",
        "    test_x['{}_mul'.format(column)] = test_x[\"year\"] * (test_x[column] + np.finfo(float).eps)\n",
        "\n",
        "#欠損値を中央値で埋める\n",
        "for col in train_x.columns:\n",
        "  median = train_x[col].median()\n",
        "  train_x[col].fillna(median, inplace=True)\n",
        "  test_x[col].fillna(median, inplace=True)\n",
        "\n",
        "#標準化\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_x)\n",
        "\n",
        "train_x = scaler.transform(train_x)\n",
        "test_x = scaler.transform(test_x)\n",
        "\n",
        "#DataFrame化\n",
        "train_x = pd.DataFrame(train_x)\n",
        "test_x = pd.DataFrame(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVId6xxUOng4"
      },
      "source": [
        "# 一層目"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG0JpMXROq8K"
      },
      "outputs": [],
      "source": [
        "class Model1NN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 200\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_shape=(train_x.shape[1],)))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(256))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(128))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(64))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(32))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(16))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "class Model2NN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 100\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(512, input_shape=(train_x.shape[1],)))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(256))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(256))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(128))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(64))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(32))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(16))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "class Model3NN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 100\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_shape=(train_x.shape[1],)))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.1))\n",
        "\n",
        "        model.add(Dense(16))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(1))\n",
        "\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "class Model4NN:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 200\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_shape=(train_x.shape[1],)))\n",
        "        model.add(PReLU())\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(.05))\n",
        "\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "class ModelRF:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 100\n",
        "\n",
        "        model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, verbose=0)\n",
        "        model.fit(tr_x, tr_y)\n",
        "\n",
        "\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "\n",
        "def predict_cv(model, train_x, train_y, test_x):\n",
        "    preds = []\n",
        "    preds_test = []\n",
        "    va_idxes = []\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=71)\n",
        "\n",
        "    # クロスバリデーションで学習・予測を行い、予測値とインデックスを保存する\n",
        "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
        "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
        "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
        "        model.fit(tr_x, tr_y, va_x, va_y)\n",
        "        pred = model.predict(va_x)\n",
        "        pred_in = model.predict(tr_x)\n",
        "        print(mean_absolute_percentage_error(tr_y, pred_in))\n",
        "        preds.append(pred)\n",
        "        pred_test = model.predict(test_x)\n",
        "        preds_test.append(pred_test)\n",
        "        va_idxes.append(va_idx)\n",
        "\n",
        "    # バリデーションデータに対する予測値を連結し、その後元の順序に並べ直す\n",
        "    va_idxes = np.concatenate(va_idxes)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    order = np.argsort(va_idxes)\n",
        "    pred_train = preds[order]\n",
        "\n",
        "    # テストデータに対する予測値の平均をとる\n",
        "    preds_test = np.mean(preds_test, axis=0)\n",
        "\n",
        "    return pred_train, preds_test\n",
        "\n",
        "\n",
        "model_1a = Model1NN()\n",
        "pred_train_1a, pred_test_1a = predict_cv(model_1a, train_x, train_y, test_x)\n",
        "pred_train_1a = pred_train_1a.flatten()\n",
        "pred_test_1a = pred_test_1a.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_1a))\n",
        "\n",
        "model_1b = Model2NN()\n",
        "pred_train_1b, pred_test_1b = predict_cv(model_1b, train_x_ps, train_y_ps, test_x)\n",
        "pred_train_1b = pred_train_1b.flatten()\n",
        "pred_test_1b = pred_test_1b.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_1b))\n",
        "\n",
        "model_1c = Model3NN()\n",
        "pred_train_1c, pred_test_1c = predict_cv(model_1c, train_x_ps, train_y_ps, test_x)\n",
        "pred_train_1c = pred_train_1c.flatten()\n",
        "pred_test_1c = pred_test_1c.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_1c))\n",
        "\n",
        "model_1d = Model4NN()\n",
        "pred_train_1d, pred_test_1d = predict_cv(model_1d, train_x, train_y, test_x)\n",
        "pred_train_1d = pred_train_1d.flatten()\n",
        "pred_test_1d = pred_test_1d.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_1d))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#既存の予測データの読み込み\n",
        "\n",
        "# def move1index(df):\n",
        "#   first_data = df.columns.values\n",
        "#   # first_data[0] = float(first_data[0])\n",
        "#   # first_data[1] = float(first_data[1])\n",
        "#   df = df.values\n",
        "#   df = np.vstack((first_data, df))\n",
        "#   df = pd.DataFrame(df)\n",
        "#   df = df.rename(columns={df.columns[0]: '', df.columns[1]: 'price'})\n",
        "#   return np.array([float(x[1:-1]) for x in df['price'].values], dtype=np.float32)\n",
        "#   # return df\n",
        "\n",
        "# pred_train_1a = pd.read_csv(\"/content/drive/MyDrive/nn1_train.csv\")\n",
        "# pred_train_1b = pd.read_csv(\"/content/drive/MyDrive/nn2_train.csv\")\n",
        "# pred_train_1c = pd.read_csv(\"/content/drive/MyDrive/nn3_train.csv\")\n",
        "# pred_train_1d = pd.read_csv(\"/content/drive/MyDrive/nn4_train.csv\")\n",
        "# pred_test_1a = pd.read_csv(\"/content/drive/MyDrive/nn1_test.csv\")\n",
        "# pred_test_1b = pd.read_csv(\"/content/drive/MyDrive/nn2_test.csv\")\n",
        "# pred_test_1c = pd.read_csv(\"/content/drive/MyDrive/nn3_test.csv\")\n",
        "# pred_test_1d = pd.read_csv(\"/content/drive/MyDrive/nn4_test.csv\")\n",
        "\n",
        "# pred_train_1a = move1index(pred_train_1a)\n",
        "# pred_train_1b = move1index(pred_train_1b)\n",
        "# pred_train_1c = move1index(pred_train_1c)\n",
        "# pred_train_1d = move1index(pred_train_1d)\n",
        "# pred_test_1a = move1index(pred_test_1a)\n",
        "# pred_test_1b = move1index(pred_test_1b)\n",
        "# pred_test_1c = move1index(pred_test_1c)\n",
        "# pred_test_1d = move1index(pred_test_1d)\n",
        "\n",
        "hex_train_pred = pd.read_csv(\"/content/drive/MyDrive/train_an_lightGBM2.csv\")\n",
        "hex_test_pred = pd.read_csv(\"/content/drive/MyDrive/en_lightGBM2.csv\")\n",
        "# # hex_train_pred2 = pd.read_csv(\"/content/train_an_lightGBM_dart.csv\")\n",
        "# # hex_test_pred2 = pd.read_csv(\"/content/en_lightGBM_dart.csv\")\n",
        "# hex_train_pred = hex_train_pred[\"price\"].values\n",
        "hex_train_pred2 = hex_train_pred2[\"price\"].values\n",
        "hex_test_pred = hex_test_pred[\"price\"].values\n",
        "# # hex_test_pred2 = hex_test_pred2[\"price\"].values"
      ],
      "metadata": {
        "id": "VQqij8VBl6J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 二層目"
      ],
      "metadata": {
        "id": "jcdxatp2Tonr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2層目のモデル\n",
        "# pred_train_2は、2層目のモデルの学習データのクロスバリデーションでの予測値\n",
        "# pred_test_2は、2層目のモデルのテストデータの予測値\n",
        "\n",
        "train_x_2_a = pd.DataFrame({'pred_1a': pred_train_1a, 'pred_1b': pred_train_1b, 'pred_1c': pred_train_1c, 'pred_1d': pred_train_1d, 'pred_1e': hex_train_pred})\n",
        "test_x_2_a = pd.DataFrame({'pred_1a': pred_test_1a, 'pred_1b': pred_test_1b, 'pred_1c': pred_test_1c, 'pred_1d': pred_test_1d, 'pred_1e': hex_test_pred})\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_x_2_a)\n",
        "\n",
        "train_x_2_a = pd.DataFrame(scaler.transform(train_x_2_a))\n",
        "test_x_2_a = pd.DataFrame(scaler.transform(test_x_2_a))\n",
        "\n",
        "\n",
        "class Model2simple:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 100\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(1, input_shape=(train_x_2_a.shape[1],)))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.1))\n",
        "\n",
        "        # model.add(Dense(256))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.1))\n",
        "\n",
        "        # model.add(Dense(128))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.05))\n",
        "\n",
        "        # model.add(Dense(64))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.05))\n",
        "\n",
        "        # model.add(Dense(32))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.05))\n",
        "\n",
        "        # model.add(Dense(16))\n",
        "        # model.add(PReLU())\n",
        "        # model.add(BatchNormalization())\n",
        "        # model.add(Dropout(.05))\n",
        "\n",
        "        # model.add(Dense(1))\n",
        "\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "\n",
        "class Model2LGBM:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "        params = {\n",
        "            'objective': 'mape',\n",
        "            'metric': 'mape',\n",
        "            'random_state': 42,\n",
        "            'max_depth': 7,\n",
        "            'n_estimators': 10000,\n",
        "            'verbosity': -1,\n",
        "            'early_stopping_round': 10,\n",
        "        }\n",
        "        num_round = 10\n",
        "        dtrain = lgbm.Dataset(tr_x, label=tr_y)\n",
        "        dvalid = lgbm.Dataset(va_x, label=va_y, reference=dtrain)\n",
        "        self.model = lgbm.train(params, dtrain, num_round, valid_sets=[dtrain, dvalid])\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "\n",
        "model_2 = Model2simple()\n",
        "pred_train_2a, pred_test_2a = predict_cv(model_2, train_x_2_a, train_y, test_x_2_a)\n",
        "pred_train_2a = pred_train_2a.flatten()\n",
        "pred_test_2a = pred_test_2a.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_2a))\n",
        "# model_2 = Model1LGBM()\n",
        "# pred_train_2b, pred_test_2b = predict_cv(model_2, train_x_2_a, train_y, test_x_2_a)\n",
        "# print(mean_absolute_percentage_error(train_y, pred_train_2b))"
      ],
      "metadata": {
        "id": "fiU5iksV1xun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#レイヤーの重み\n",
        "model_2.model.get_weights()"
      ],
      "metadata": {
        "id": "D2RSkh9mPKpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#特徴量相関\n",
        "train_x_2_a.corr()"
      ],
      "metadata": {
        "id": "LQad4CyT_qGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utput_file = \"二値分類_test.csv\"\n",
        "# with open(utput_file, 'w') as f:\n",
        "#     for i in range(len(test)):\n",
        "#         row = f\"{test['id'][i]},{pred_test_2a[i]}\\n\"\n",
        "#         f.write(row)\n",
        "\n",
        "# utput_file = \"二値分類_train.csv\"\n",
        "# with open(utput_file, 'w') as f:\n",
        "#     for i in range(len(train)):\n",
        "#         row = f\"{train['id'][i]},{pred_train_2a[i]}\\n\"\n",
        "#         f.write(row)"
      ],
      "metadata": {
        "id": "Cn-s5Lwi10bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33RFPZywZAkV"
      },
      "source": [
        "# 三層目"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wlz-cVmQYs0i"
      },
      "outputs": [],
      "source": [
        "# pred_train_2a = pd.read_csv(\"/content/drive/MyDrive/NNandLGBM_NN_train (1).csv\")\n",
        "# pred_test_2a = pd.read_csv(\"/content/drive/MyDrive/NNandLGBM_NN_test (1).csv\")\n",
        "hex_train_pred = pd.read_csv(\"/content/drive/MyDrive/train_en_light_nn_2.csv\")\n",
        "hex_test_pred = pd.read_csv(\"/content/drive/MyDrive/en_light_nn2.csv\")\n",
        "# hex_train_pred2 = pd.read_csv(\"/content/drive/MyDrive/train_an_lightGBM2.csv\")\n",
        "# hex_test_pred2 = pd.read_csv(\"/content/drive/MyDrive/en_lightGBM2.csv\")\n",
        "\n",
        "# first_data = katoro_train_pred.columns.values\n",
        "# first_data[1] = float(first_data[1])\n",
        "# katoro_train_pred = katoro_train_pred.values\n",
        "# katoro_train_pred = np.vstack((first_data, katoro_train_pred))\n",
        "# katoro_train_pred = pd.DataFrame(katoro_train_pred)\n",
        "# katoro_train_pred = katoro_train_pred.rename(columns={0: '', 1: 'price'})\n",
        "\n",
        "# first_data = katoro_test_pred.columns.values\n",
        "# first_data[1] = float(first_data[1])\n",
        "# katoro_test_pred = katoro_test_pred.values\n",
        "# katoro_test_pred = np.vstack((first_data, katoro_test_pred))\n",
        "# katoro_test_pred = pd.DataFrame(katoro_test_pred)\n",
        "# katoro_test_pred = katoro_test_pred.rename(columns={0: '', 1: 'price'})\n",
        "\n",
        "# pred_train_2a = pred_train_2a[\"price\"].values\n",
        "hex_train = hex_train_pred[\"price\"].values\n",
        "# pred_test_2a = pred_test_2a[\"price\"].values\n",
        "hex_test = hex_test_pred[\"price\"].values\n",
        "# hex_train2 = hex_train_pred2[\"price\"].values\n",
        "# hex_test2 = hex_test_pred2[\"price\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPl7LiUuhXp9"
      },
      "outputs": [],
      "source": [
        "train_x_3 = pd.DataFrame({'pred_1d': pred_train_2a, 'pred_1e': hex_train})\n",
        "test_x_3 = pd.DataFrame({'pred_1d': pred_test_2a, 'pred_1e': hex_test})\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_x_3)\n",
        "\n",
        "train_x_3 = pd.DataFrame(scaler.transform(train_x_3))\n",
        "test_x_3 = pd.DataFrame(scaler.transform(test_x_3))\n",
        "\n",
        "\n",
        "class Model3simple:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "\n",
        "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
        "\n",
        "        batch_size = 64\n",
        "        epochs = 300\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        # model.add(Dense(32, input_shape=(train_x_2.shape[1],)))\n",
        "        # model.add(PReLU())\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "        adam = optimizers.Adam(lr=0.2, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        sgd = optimizers.SGD(learning_rate=0.03)\n",
        "        model.compile(optimizer=adam, loss='mean_absolute_percentage_error', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "        history = model.fit(tr_x, tr_y, epochs=epochs, batch_size=batch_size, verbose=1, validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def predict(self, x):\n",
        "        pred = self.model.predict(x)\n",
        "        return pred\n",
        "\n",
        "model_3 = Model3simple()\n",
        "pred_train_3, pred_test_3 = predict_cv(model_3, train_x_3, train_y, test_x_3)\n",
        "pred_train_3 = pred_train_3.flatten()\n",
        "pred_test_3 = pred_test_3.flatten()\n",
        "print(mean_absolute_percentage_error(train_y, pred_train_3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#レイヤー重み\n",
        "model_3.model.get_weights()"
      ],
      "metadata": {
        "id": "KDLUthE7YLFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0HPxlSbh7ng"
      },
      "outputs": [],
      "source": [
        "#特徴量相関\n",
        "train_x_3.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITuvcR72iBsk"
      },
      "outputs": [],
      "source": [
        "# 最終予測出力\n",
        "# output_file = \"うんち博打二号.csv\"\n",
        "# with open(output_file, 'w') as f:\n",
        "#     for i in range(len(test)):\n",
        "#         row = f\"{test['id'][i]},{pred_test_2[i]}\\n\"\n",
        "#         f.write(row)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1f0cbW-f5kOgUitBIV0I_xZHiYhNM9Apj",
      "authorship_tag": "ABX9TyPPuyJBIcVsGbnZeqa8wv+W",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}